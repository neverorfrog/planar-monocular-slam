<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Planar Monocular SLAM</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1, h2, h3, h4 { color: #2c3e50; }
        h1 { border-bottom: 2px solid #3498db; padding-bottom: 10px; }
        h2 { border-bottom: 1px solid #bdc3c7; padding-bottom: 5px; }
        h3 { margin-top: 25px; }
        h4 { margin-top: 20px; color: #34495e; }
        code { background-color: #f8f9fa; padding: 2px 4px; border-radius: 3px; font-family: 'Monaco', 'Consolas', monospace; }
        pre { background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }
        ul { list-style-type: disc; margin-left: 20px; }
        ol { margin-left: 20px; }
        img { max-width: 100%; height: auto; display: block; margin: 20px auto; border-radius: 5px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .error-metrics { background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 15px 0; border-left: 4px solid #3498db; }
        .results { background-color: #d4edda; padding: 15px; border-radius: 5px; margin: 15px 0; border-left: 4px solid #28a745; }
        .triangulation-results { background-color: #fff3cd; padding: 15px; border-radius: 5px; margin: 15px 0; border-left: 4px solid #ffc107; }
        a { color: #3498db; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .math-block { margin: 15px 0; }
        .algorithm-step { background-color: #f8f9fa; padding: 10px; margin: 10px 0; border-radius: 5px; border-left: 3px solid #6c757d; }
    </style>
</head>
<body>
    <h1>Planar Monocular Slam</h1>
    
    <p>Project of the Probabilistic Robotics Course at Sapienza University of Rome. The goal is to develop a planar monocular slam pipeline, given a dataset made of noisy odometry and measurement made by a monocular camera. The goal is to estimate the position of the perceived landmarks and correct the drift in the odometry, based on these observations.</p>

    <h2>Problem</h2>
    <p>Differential Drive equipped with a monocular camera.</p>

    <h3>Input</h3>
    <ul>
        <li>Integrated dead reckoning (wheeled odometry)</li>
        <li>Stream of point projections with "id"</li>
        <li>Camera parameters
            <ul>
                <li>Extrinsics (pose of camera on robot)</li>
                <li>Intrinsics (K)</li>
            </ul>
        </li>
    </ul>

    <h3>Output</h3>
    <ul>
        <li>Trajectory (estimate vs gt)</li>
        <li>3D points (estimate vs gt)</li>
        <li>Error values (rotation and translation)</li>
    </ul>

    <p>The solution can be divided in two macro-steps: triangulation and bundle adjustment.</p>

    <h2>Triangulation [1]</h2>

    <p><strong>Given:</strong></p>
    <ul>
        <li>A set of $M \ge 2$ 2D image projections $\mathbf{x}_j = [u_j, v_j]^T$ for a single landmark, observed from $M$ different camera views.</li>
        <li>The camera intrinsic matrix $K$ (3x3).</li>
        <li>The camera extrinsics relative to the robot: $T_{RC}$ (a 4x4 transformation from camera frame to robot frame).</li>
        <li>The robot's odometry pose for each view $j$: $T_{WR}^{(j)}$ (a 4x4 transformation from robot frame to world frame).</li>
    </ul>

    <p>We want to find the 3D position of the landmark in homogeneous world coordinates, $\mathbf{X}_w = [X, Y, Z, 1]^T$.</p>

    <div class="math-block">
        <p>The projection equation implies that $\mathbf{x}_{img, j}$ and $P^{(j)} \mathbf{X}_w$ are collinear. Thus, their cross product is zero [1, Slides 15-21]:</p>
        <p>$$\mathbf{x}_{img, j} \times (P^{(j)} \mathbf{X}_w) = \mathbf{0}$$</p>
    </div>

    <p>Substituting $\mathbf{x}_{img, j} = [u_j, v_j, 1]^T$ and $P^{(j)} \mathbf{X}_w = \begin{pmatrix} \mathbf{p}_{j1}^T \mathbf{X}_w \\ \mathbf{p}_{j2}^T \mathbf{X}_w \\ \mathbf{p}_{j3}^T \mathbf{X}_w \end{pmatrix}$, the cross product yields three equations, two of which are linearly independent:</p>

    <ol>
        <li>$(v_j \mathbf{p}_{j3}^T - \mathbf{p}_{j2}^T) \mathbf{X}_w = 0$</li>
        <li>$(\mathbf{p}_{j1}^T - u_j \mathbf{p}_{j3}^T) \mathbf{X}_w = 0$</li>
    </ol>

    <div class="math-block">
        <p>For $M$ observations, we stack these $2M$ equations into a single homogeneous linear system:</p>
        <p>$$A \mathbf{X}_w = \mathbf{0}$$</p>
        <p>where $A$ is a $(2M \times 4)$ matrix. Since measurements are noisy, this system is solved in a least-squares sense to find $\mathbf{X}_w$ that minimizes $||A \mathbf{X}_w||^2$.</p>
    </div>

    <h3>Robust Landmark Triangulation</h3>
    <p>Actually, triangulation over all measurements turned out to give bad results. Instead, what gave better results was using a RANSAC-like approach.</p>
    
    <div class="algorithm-step">
        <p>For each consecutive frame pair $(i, i+1)$:</p>
        <ol>
            <li>Compute camera poses: ${}^CT_{W}^{(i)}$ and ${}^CT_{W}^{(i+1)}$</li>
            <li>Identify landmarks visible in both frames</li>
            <li>Extract corresponding 2D measurements: $\mathbf{x}_i = [u_i, v_i]^T$ and $\mathbf{x}_{i+1} = [u_{i+1}, v_{i+1}]^T$</li>
        </ol>
    </div>

    <p>Then, For each landmark visible in both consecutive frames, the method applies the Direct Linear Transform (DLT) algorithm using only two views.</p>

    <div class="math-block">
        <p>This results in a $(4 \times 4)$ linear system:</p>
        <p>$$A \mathbf{X}_w = \mathbf{0}$$</p>
    </div>

    <p>Each triangulated point undergoes strict outlier detection using the <code>isOutlier</code> method:</p>

    <ol>
        <li><strong>Cheirality Check</strong>: Verify the point is in front of both cameras
            <ul>
                <li>Transform point to camera coordinates: $\mathbf{p}_{cam} = T_{CW} \mathbf{X}_w$</li>
                <li>Check: $Z_{cam} > 0$ and $Z_{cam} \leq z_{far}$</li>
            </ul>
        </li>
        <li><strong>Reprojection Error Check</strong>:
            <ul>
                <li>Reproject the 3D point back to 2D: $\mathbf{x}_{reproj} = K [R|t] \mathbf{X}_w$</li>
                <li>Compute error: $||\mathbf{x}_{reproj} - \mathbf{x}_{measured}||_2$</li>
                <li>Reject if error $> 10.0$ pixels</li>
            </ul>
        </li>
    </ol>

    <div class="math-block">
        <p>For each landmark, the algorithm collects multiple triangulation estimates from different consecutive frame pairs where the landmark was visible. The final 3D position is computed as:</p>
        <p>$$\mathbf{X}_{final} = \frac{1}{N} \sum_{k=1}^{N} \mathbf{X}_k$$</p>
        <p>where $N$ is the number of valid triangulation estimates that passed the outlier filtering.</p>
    </div>

    <h2>Bundle Adjustment</h2>
    <p>Once the initial guess about the map is given, the optimization process can start. The state we wanna estimate is made of 2D robot poses and landmark positions. The bundle adjustement approach can be divide into two steps:</p>
    <ul>
        <li>Linearization</li>
        <li>Least Squares</li>
    </ul>

    <h3>Linearization</h3>
    <p>This consists in computing the hessian and gradient of the error function, having the measurement (aka an image point, already associated to the landmark), the guessed landmark (from triangulation) and the pose of the camera with respect to the world frame.</p>
    <p>The result of this process are two matrices, $H$ and $b$, which are then used to find a $\delta$ to add to the state, in order to, ideally, go near the ground truth.</p>
    <p>The two matrices are constructed by iterating over all measurements, and for each corresponding pair of pose n and landmark m do the following.</p>

    <ol>
        <li>Landmark from homogeneus to camera coordinates: $ {}^c p_m = P \cdot {}^w p_m $ where $ P = K \cdot [{}^c R_w | {}^c t_w] $</li>
        <li>Prediction $h$ obtained by dividing the point in camera coordinates by the last component and extracting the first two components</li>
        <li> 
            Compute the error $e_{nm} = h - z$, where $z$ is the measurement</li>
        </li>
        <li>Jacobians of the error with respect to $\Delta x_r$ and $\Delta x_l$ computed as follows
            <ul>
                <li>$ J_r = J_{proj} \cdot K \cdot J_{icpr}$</li>
                <li>$ J_l = J_{proj} \cdot K \cdot J_{icpl}$</li>
            </ul>
        </li>
        <li>
            Insert $J_r$ and $J_l$ into $J_{nm}$ at the appropriate indices.
        </li>
        <li>Update the hessian and gradient base on jacobian $J_{nm}$ and $e_{nm}$:
            <ul>
                <li>$H = H + J_{nm}^T \cdot J_{nm}$</li>
                <li>$b = b + J_{nm}^T \cdot e_{nm}$</li>
            </ul>
        </li>
    </ol>

    <h4>Derivation of $J_{icpr}$</h4>
    <div class="math-block">
        <p>The expression for the landmark's position in the camera frame, namely the measurement, factoring out the projection operation, is:</p>
        <p>$$ {}^c p_m = {}^c R_w \cdot {}^w p_m - {}^c R_w \cdot {}^w t_r + {}^c t_r = {}^c R_w(\theta) \cdot ({}^w p_m - {}^w t_r(x,y)) + {}^c t_r$$</p>
        
        <p>The derivative $J_{icpr}$ will be a 3x3 matrix:</p>
        <p>$$J_{icpr} = \left[ \frac{d({}^c p_m)}{dx} \quad \frac{d({}^c p_m)}{dy} \quad \frac{d({}^c p_m)}{d\theta} \right]$$</p>
        
        <p>The derivatives with respect to each element of the 2D pose are:</p>
        <ul>
            <li>$\frac{d({}^c p_m)}{dx} = -{}^c R_w(\theta) \cdot [1, 0, 0]^T$</li>
            <li>$\frac{d({}^c p_m)}{dx} = -{}^c R_w(\theta) \cdot [0, 1, 0]^T$</li>
            <li>$\frac{d({}^c p_m)}{d\theta} = ({}^c R_r \cdot \dot{{}^r R_w}(\theta)) \cdot ({}^w p_m - {}^w t_r(x,y))$</li>
        </ul>
    </div>

    <h4>Derivation of $J_{icpl}$</h4>
    <p>This one is simply $J_{icpl} = {}^c R_w(\theta)$</p>

    <h3>Robust Kernel</h3>
    <p>Using $L2_{trunc}$ with a kernel threshold of $3\cdot 10^3$ gave better results</p>

    <h2>Results</h2>

    <h3>Triangulation</h3>
    <p>With maximum reprojection error at 20.0</p>
    
    <div class="triangulation-results">
        <ul>
            <li>Root Mean Square Error (RMSE): 1.3098</li>
            <li>Mean Absolute Error (MAE): 0.5222</li>
            <li>MAE per coordinate: X=0.7557, Y=0.6633, Z=0.1475</li>
            <li>Mean Euclidean Distance: 1.1222</li>
            <li>Median Euclidean Distance: 0.9179</li>
        </ul>

        <p><strong>Component-wise errors:</strong></p>
        <ul>
            <li>X-axis: Mean=0.2404, Std=0.9256, Min=-3.1103, Max=2.3314</li>
            <li>Y-axis: Mean=-0.1812, Std=0.8519, Min=-2.6715, Max=3.4090</li>
            <li>Z-axis: Mean=0.1406, Std=0.1511, Min=-0.1913, Max=0.7830</li>
        </ul>

        <p><strong>Valid landmarks:</strong> 783/1000 (78.3%)</p>
    </div>

    <h3>Bundle Adjustment</h3>
    <div class="results">
        <p><strong>After 38 iterations:</strong></p>
        <ul>
            <li>Map RMSE $ = 0.0775 $</li>
            <li>Linear Trajectory Error $ = 0.1060 $</li>
            <li>Angular Trajectory Error $ = 0.0219 $</li>
        </ul>
    </div>

    <img src="animations/landmarks_animation.gif" alt="Landmarks Animation - SLAM in action">
    <img src="animations/trajectory_animation.gif" alt="Trajectory Animation - SLAM in action">
    <img src="figures/errors.png" alt="Error Plot">
    <img src="figures/inliers.png" alt="Inlier Plot">

    <p><strong>Comments:</strong></p>
    <ul>
        <li>The ground truth landmarks that are not superposed by an estimate are the ones that are never seen in any measurement.</li>
        <li>Linear error is not perfect, could probably enhanced by better outlier detection searching for good parameters or an even better initial guess</li>
        <li>Pose-Pose constraint didn't help</li>
    </ul>

    <h3>References</h3>
    <p>[1] Kris Kitani, "Triangulation," 16-385 Computer Vision, Carnegie Mellon University. [Online]. Available: <a href="https://www.cs.cmu.edu/~16385/s17/Slides/11.4_Triangulation.pdf">https://www.cs.cmu.edu/~16385/s17/Slides/11.4_Triangulation.pdf</a></p>
</body>
</html>